{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Chapter 7: The Pattern Package"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`Pattern` (see http://www.clips.ua.ac.be/pages/pattern) is a versatile Python software package written by Tom De Smedt from the CLiPS lab at the University of Antwerp (Belgium). This easy-to-use package is distributed over a number of modules that provide functionality for mining information from the web, for processing natural language, as well as for applying Machine Learning and Statistics. Pattern is very similar to a number of other more specialized Python packages, such as the `Natural Language Toolkit` or `NLTK`, which focuses on linguistics (see http://www.nltk.org), `matplotlib` (see http://matplotlib.org/), which allows you to visualize scientific data or `scikit-learn`, which foregrounds techniques for Machine Learning (see http://www.scikit-learn.org).\n",
      "\n",
      "However, what makes `Pattern` unique is its general ease-of-use and its unparalleled web mining module (pattern.web) which make it a very useful starting point for novice programmers. However, if you mean to continue to code in Python, we do recommend that you check these related packages too, since you will quickly find out that these packages might offer functionality that is currently perhaps lacking from Pattern. Note that Pattern still uses Python 2.5+, instead of the more recent Python 3 version we have been working with so far.\n",
      "\n",
      "Below you will find a series of exercises that you can use to familiarize yourself with the Pattern package. Note, however, that the scope of the package is much broader than the topics we will able to discuss today. You might to store your code for each of these exercices below, since you might need portions of it later again. The package is well-documented online, so please make good use of this documentation while making the exercices! Don't forget to use Python's `help()` function, whenever you are not sure about e.g. what kind of object a new function returns. Happy coding!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Pattern.web\n",
      "\n",
      "* Mine 500 tweets from Twitter searching for the hashtag \"#pepsi\" and 500 tweets \"#CocaCola\".\n",
      " - How many tweets can you get per search query through the Pattern API?  \n",
      " - Do you only get English-language tweets? Is there an easy way to filter out tweets in other languages?\n",
      " - Use the hashtags function to extract Twitter keywords that occur both in the Pepsi and Coke tweets.\n",
      "* Search for tweets containing the hashtag of our summer school (\"#GoeDH2013\"). Count how many tweets were posted by Mike (@Mike_Kestemont). Fill a tuples with the dates on which these were posted.\n",
      "* Use Pattern to search Wikipedia for information on \"Goettingen\":\n",
      " - How many search results does the search function return in this case?\n",
      " - Extract the search results for Dutch, German and French too.\n",
      " - What is the type of the object being returned? Use the `help()` function to display information about this object.\n",
      " - How many sections does the German page contain?\n",
      " - Print the plain text contained in the fourth section on the French page.\n",
      "* Retrieve the English Wikipedia-pages on Aristoteles, Socrates, Plato, and Alexander the Great. Do all these pages link to one another? Save to a file a list the (lowercased) words that are common to all the sections in these four pages (use the `plaintext` property).\n",
      "* Search for the Wikipedia page on \"Joker\". Is there only one page being returned by Pattern's search function? Which boolean property can you use to check for this in Pattern? Use the article's `links` property to iteratively retrieve the text of the other articles to which this page links. How many of these pages link to Wikipedia's page on \"Batman\"?\n",
      "* Find out out which address you can use to retrieve the RSS feed of the famous Science journal (see http://www.sciencemag.org). Collect the 5 most recent RSS feeds made by the journal. These titles typically have a lot of difficult words in them: what is the percentage of words that have an English Wikipedia-page devoted to them? Filter out very common words, such \"the\" or \"and\".\n",
      "\n",
      "\n",
      "##Pattern.en\n",
      "\n",
      "Traditionally, \"parsing\" is one of the key topics in natural language processing. It generally refers to the full-automatic analysis of the linguistic structure of sentences.\n",
      "\n",
      "* Use the `parse()` function with default parameters to analyze the following sentence: \"Anthony used to eat soup with a fork and not with a spoon\".\n",
      " - Store the resulting object and print it using the standard `print()` function.\n",
      " - Now print it with Pattern's built-in 'pretty print' function `pprint()`. (Note: you will probably get an error message: you have to import this function first!) Have a look at the part-of-speech tags using the documentation: did Pattern manage to disambiguate all tags correctly? What about its parse of the ambiguous sentences: \"He can drink from a can.\" or \"The old men the boat.\"?\n",
      " - Now write the part-of-speech tags of the first sentence to a file, after having extracted them using the `tag()` function.\n",
      "- Now re-parse the sentence, adding lemmas (= dictionary headforms) and (syntactic) relations to the ambiguous sentences: how does that work out?\n",
      " - Note that Pattern results are not always state-of-the-art when it comes to linguistic accuracy, but it does a good trick and is very fast. The use of the English language online, however, is quite different from the data on which Pattern was \"trained\". Often parsing yields results for this kind of data that are not very accurate.\n",
      "- Can you download 50 tweets (in the English language!) containing the word \"can\" and \"#Fanta\" and parse them for part-of-speech using the `tag()` function. In how many cases has the word \"can\" been correctly tagged?\n",
      "- Now, make a new parse that also has lemmas: can you identify a number of words that have been wrongly lemmatized?\n",
      "- Why might this have happened? Import Pattern's `spelling` module and use the `suggest()` function from it to find suitable spelling alternative for these wrongly spelled words. Does this actually work for any of the words?\n",
      "* Chunking is an interesting application of NLP since it allows to extract, for instance, noun phrases, like \"the beautiful cat\" or \"a poisoned apple\". Open the Wikipedia page on Lady Gaga (the beautiful singer) and `parse()` the entire text of the article for chunks.\n",
      "- What is the type of the object returned? Can you loop over the object? What are the different hierarchical levels in this object?\n",
      "- Make a separate list of all Noun Phrases that contain at least one adjective on this page.\n",
      "- Make a list of all chunks that contain the word \"all\".\n",
      "- Make a list of all object phrases on this page: count how many the word \"music\" figures as an object in these.\n",
      "* Let's do a bit of work on \"sentiment mining\", a very popular business application of Natural Language Processing. \n",
      "- Collect 1000 tweets on \"#Microsoft\" and 1000 tweets on \"#Apple\".\n",
      "- Parse these tweets and collect a frequency dictionary of all the adjectives used in both tweet categories.\n",
      "- Now compare the tweets' overall sentiment by summing the result of the `pattern.en.sentiment()` function (for sentiment, i.e. not polarity!) for each item in both brands' adjective dictionaries -- don't forget to take into account the frequency of adjectives in your dictionary. About which brand were people on Twitter most positive in your tweet collection?\n",
      "- Now add adverbs from your tweet collections to your frequency dictionaries and have a look at the overall \"polarity\" of these items. For which brand do these tweets show the highest polarity?\n",
      "- Now forget about the parse -- which probably wasn't too correct anyways? -- and collect the sentiment and polarity stats for all words. Does another trend emerge?\n",
      "* Read in the Wikipedia pages on Al Gore, Steve Jobs, George Bush, Sergei Brin and Barack Obama. Compare the global sentiment in these articles -- note that you can also pass an entire string to `sentiment()`, instead of just individual words. Can you rank these famous people according to the sentiment expressed in these articles?\n",
      "* Another great tool for the NLP analysis of textual data is `WordNet`, a large structured collection of words in the English language that identifies relationships between words such as synonyms (words that have a similar meaning) or antonyms (words that have a contradictory meaning). Use your you Microsoft vs. Apple tweet collection: re-tokenize the original #Microsoft-tweets, loop over the tokens and find out which words have at least one antonym which is present in the tokenized #Apple Collection. Then repeat this process the other way round.\n",
      "* Open the Wikipedia page on \"Mammals\" and extract the titles of the other pages to which this page links.\n",
      "- Check via your code whether any of these items also occurs in the list of \"meronyms\" which WordNet lists for \"Mammal\".\n",
      "- Use Python code to find out whether the Wikipedia page on dog explicitly links to any of the \"holonyms\" which WordNet lists for \"Dog\".\n",
      "\n",
      "\n",
      "##Pattern.vector\n",
      "\n",
      "Download the `20newsgroups`dataset from the following location http://qwone.com/~jason/20Newsgroups/. Unpack it and place it in your working directory. This is a corpus which is normally used as a data set for a well-known task called 'topic detection': the task is to train a machine to able to automatically recognize the topic of a text. This is a really helpful application (for instance, for an online newspaper that might want to automatically assign the updates in its RSS feeds to one of the paper's main categories, such as sports vs. finance. Subscribers can then easily filter out feeds about topics which they don't care really about). \n",
      "\n",
      "This classification task is of course related to \"topic modeling\" (e.g. via Latent Dirichlet Allocation), with the exception that we will be assuming for the sake of classification that a document can only have a single main topic. Note that the `20_newsgroups` data set has a number of broader categories, such as rec(reation) and comp(uter), indicated by a prefix in the directory's name. Most of these broader groups have multiple subcategories, such as \"guns\", \"mideast\" and \"misc\" for \"talk.politics\".\n",
      "\n",
      "\n",
      "* Make a copy (in code, not manually!) of the original `20_newgroups` directory structure under \"20_Newsgroups.clean\". Create empty copies of the original subdirs under 20_newsgroups.clean and add \".clean\" to the names of the new subdirs (e.g. talk.politics.mideast has to become talk.politics.mideast.clean). Check whether a directory already exists before attempting to create it. When you inspect the text files in the original 20_newsgroups directories you will notice that they still contain meta-information that you don't want to include as actual text data in the classification task. Write a function to automatically remove this meta-information (up to the line starting with \"Lines:\" etc.) and do not include files that after cleaning them up contain less than 10 words. Write the clean result (i.e. containing only the actual text) to a new file under the appropriate new \".clean\"-directory. Append the extension \".clean.txt\" to the original file name. For this (difficult!) exercise, you will want to make use of functions such as \"walk\", \"mkdir\" and \"exists\" in Python's standard \"os\" module from the standard library (Use the online documentation!).\n",
      "* Under the newly created subdir \"soc.religion.christian.clean\", you should now find a file called \"20361.clean.txt\".\n",
      " - Create an new `Document` object on the basis of the contents of \"20361.clean.txt\". Set its name property to the original file name and its \"type\" to \"religion\", since that should be its topic.\n",
      " - Via which property can you now access the original words in the document? Does a `Document`-object contain Sentence-objects by default?\n",
      " - The `Document` object operates under a bags-of-words assumption: can you figure out yourself what this means as to the original word order of the document?\n",
      " - Reload the file but set `stopwords` to \"True\" in the document's constructor. What has happened? Extract the documents top-5 keywords: do these make sense?\n",
      " - Reload the document but use the LEMMA stemmer this time.\n",
      " - Reload the file without the stopwords but remove the words \"failure\", \"abstinence\" and \"course\" (using only the constructor of course).\n",
      " - Reload the file without the stopwords and re-extract the document's top-5 keywords: do they make more sense now?\n",
      " - Compare the document's current `tf` and `tfidf`: why are these the same?\n",
      "\n",
      "* Now construct an entire corpus from all the documents under the directory \"soc.religion.christian.clean\" (using the default `Document` options). Set their type to \"religion\"\n",
      " - Check again on the `tf` and `tfidf` of 'failure' in the first document: are they the same now? What has happened?\n",
      " - Add the documents to your corpus listed under \"comp.graphics\" and set their type to \"graphics\".\n",
      " - List the 50 words that have the highest \"Information Gain\". Does this list make sense to you?\n",
      " - Now print a so-called \"Confusion Matrix\" of the classification results: is there a tendency for classes from the same broader category (e.g. comp) to be confused more easily by the classifier?\n",
      "\n",
      "* Implement a 'wrapper' function around Pattern's classification services that can do \"leave-one-out validation\" on a corpus of representing text documents. This is a well-known experimental design in Machine Learning. You iteratively \"leave out\" one of the items in your original set (containing n items) and put them aside as a test object. Next, you train a classifier on the n-1 remaining training documents in your collection and then you have the newly trained machine learner classifiy the single, held-out item. You repeat this process for each item in your document set (i.e. n times) and each you store the classification result (correct or not correct). At the end of the ride, you can calculate the overall accuracy of your intelligent system (the number of correct applications divided by n): the value gives you a good idea of the generalization performance of your classifier, i.e. an idea of how well the classifier would perform if you would present it with a new, unknown item.\n",
      "- a parameter specifying the classifier to use \"NB\", \"KNN\" or \"SVM\".\n",
      "- a parameter specifiying to number of top-frequent words to use\n",
      "\n",
      "* Now apply your classifier to the text categories under the 20newsgroup data using the function which you just wrote for \"topic classification\". \n",
      "- Find out which classifier performs best on this task.\n",
      "- Write a loop to create a so-called \"learning curve\" and find out whether different settings of the number of words parameters affect the accuracy of the best performing accuracy. Use the values: 50, 100, 250, 500, 1000, 2000."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "----------------------\n",
      "\n",
      "You've reached the end of Chapter 7! Ignore the code below, it's only here to make the page pretty:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.core.display import HTML\n",
      "def css_styling():\n",
      "    styles = open(\"styles/custom.css\", \"r\").read()\n",
      "    return HTML(styles)\n",
      "css_styling()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<style>\n",
        "    @font-face {\n",
        "        font-family: \"Computer Modern\";\n",
        "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
        "    }\n",
        "    div.cell {\n",
        "        width:800px;\n",
        "        margin-left:auto;\n",
        "        margin-right:auto;\n",
        "    }\n",
        "    div.cell, .input.hbox {\n",
        "    display:block;\n",
        "}\n",
        "    h1 {\n",
        "        font-family: \"Charis SIL\", Palatino, serif;\n",
        "    }\n",
        "    h4{\n",
        "        margin-top:12px;\n",
        "        margin-bottom: 3px;\n",
        "       }\n",
        "    div.text_cell_render{\n",
        "        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
        "        line-height: 145%;\n",
        "        font-size: 120%;\n",
        "        width:800px;\n",
        "        margin-left:auto;\n",
        "        margin-right:auto;\n",
        "    }\n",
        "    .CodeMirror{\n",
        "            font-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
        "    }\n",
        "    .prompt{\n",
        "        display: None;\n",
        "    }\n",
        "    .text_cell_render h5 {\n",
        "        font-weight: 300;\n",
        "        font-size: 16pt;\n",
        "        color: #4057A1;\n",
        "        font-style: italic;\n",
        "        margin-bottom: .5em;\n",
        "        margin-top: 0.5em;\n",
        "        display: block;\n",
        "    }\n",
        "    \n",
        "    .warning{\n",
        "        color: rgb( 240, 20, 20 )\n",
        "        }\n",
        "\n",
        "    blockquote {\n",
        "    border-left: 4px solid #DDDDDD;\n",
        "    color: #777777;\n",
        "        padding: 0 15px;\n",
        "        }\n",
        "\n",
        "</style>\n",
        "<script>\n",
        "    MathJax.Hub.Config({\n",
        "                        TeX: {\n",
        "                           extensions: [\"AMSmath.js\"]\n",
        "                           },\n",
        "                tex2jax: {\n",
        "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
        "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
        "                },\n",
        "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
        "                \"HTML-CSS\": {\n",
        "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
        "                }\n",
        "        });\n",
        "</script>"
       ],
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "<IPython.core.display.HTML at 0x1024fb810>"
       ]
      }
     ],
     "prompt_number": 1
    }
   ],
   "metadata": {}
  }
 ]
}